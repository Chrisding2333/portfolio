{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33be8fd",
   "metadata": {},
   "source": [
    "# MajorityVote\n",
    "\n",
    "0. Import Necessary Packages\n",
    "1. Import Check list\n",
    "2. Import and Process Subreddit Files\n",
    "3. Label for Duplicated Data and Separate fron Unique Data\n",
    "4. Apply Majority Vote for\"out-of-State\" Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd567d",
   "metadata": {},
   "source": [
    "## 0. Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87baf989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34241bf",
   "metadata": {},
   "source": [
    "## 1. Import Check list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efa848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subreddit  City County  State  District   Place_Name State_Name\n",
      "4        s1   0.0      0    1.0       0.0  test state1         S1\n",
      "5        s2   0.0      0    1.0       0.0  test state2         S2\n",
      "   Subreddit  City County  State  District  Place_Name State_Name\n",
      "0  testcity1   1.0      0    0.0       0.0  test city1         S1\n",
      "1  testcity0   1.0      0    0.0       0.0  test city0         S1\n",
      "2  testcity2   1.0      0    0.0       0.0  test city2         S2\n",
      "3  testcity3   1.0      0    0.0       0.0  test city3         S2\n",
      "6  district0   0.0      0    0.0       1.0  test city1         S1\n"
     ]
    }
   ],
   "source": [
    "refer_df = pd.read_csv('ReferListTest.csv')\n",
    "refer_df['Subreddit'] = refer_df['Subreddit'].str.lower()\n",
    "refer_df['Place_Name'] = refer_df['Place_Name'].str.lower()\n",
    "\n",
    "refer_df['City'] = pd.to_numeric(refer_df['City'], errors='coerce')\n",
    "refer_df['District'] = pd.to_numeric(refer_df['District'], errors='coerce')\n",
    "refer_df['State'] = pd.to_numeric(refer_df['State'], errors='coerce')\n",
    "\n",
    "#generate refer list for states only\n",
    "state_refer_df = refer_df[refer_df['State'] == 1]\n",
    "print(state_refer_df)\n",
    "\n",
    "#generate refer list for cities and districts\n",
    "refer_df = refer_df[(refer_df['City'] == 1) | (refer_df['District'] == 1)]\n",
    "print(refer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71941c27",
   "metadata": {},
   "source": [
    "## 2. Import and Process Subreddit Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04dd22e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching subreddit found for s1. Skipping...\n",
      "No matching subreddit found for s2. Skipping...\n",
      "No matching subreddit found for teabag. Skipping...\n",
      "Empty in TestCity3_posts.csv. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>Place_Name</th>\n",
       "      <th>State_Name</th>\n",
       "      <th>n_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IhateS2</td>\n",
       "      <td>test city1</td>\n",
       "      <td>S1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IhateS2</td>\n",
       "      <td>test city2</td>\n",
       "      <td>S2</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IliveInS1</td>\n",
       "      <td>test city1</td>\n",
       "      <td>S1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IliveInS1&amp;neverlft</td>\n",
       "      <td>test city0</td>\n",
       "      <td>S1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IliveInS1&amp;neverlft</td>\n",
       "      <td>test city1</td>\n",
       "      <td>S1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IliveInS2</td>\n",
       "      <td>test city1</td>\n",
       "      <td>S1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IliveInS2</td>\n",
       "      <td>test city2</td>\n",
       "      <td>S2</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>random1</td>\n",
       "      <td>test city0</td>\n",
       "      <td>S1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>random2</td>\n",
       "      <td>test city0</td>\n",
       "      <td>S1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>random3</td>\n",
       "      <td>test city0</td>\n",
       "      <td>S1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  Place_Name State_Name  n_posts\n",
       "0              IhateS2  test city1         S1       99\n",
       "1              IhateS2  test city2         S2       99\n",
       "2            IliveInS1  test city1         S1      100\n",
       "4   IliveInS1&neverlft  test city0         S1       99\n",
       "5   IliveInS1&neverlft  test city1         S1       99\n",
       "6            IliveInS2  test city1         S1       99\n",
       "7            IliveInS2  test city2         S2       99\n",
       "8              random1  test city0         S1       99\n",
       "9              random2  test city0         S1       99\n",
       "10             random3  test city0         S1       99"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize empty variables.\n",
    "df_list = []\n",
    "max_posts_df = pd.DataFrame()\n",
    "\n",
    "#Process city and district files one by one.\n",
    "for filename in glob.glob('*_posts.csv'):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    #Skip for files that are empty.\n",
    "    if df.iloc[0:].empty:\n",
    "        print(f\"Empty in {filename}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    subreddit_city = os.path.splitext(filename)[0].replace('_posts', '').lower()\n",
    "    \n",
    "    #Get City's info from refer list.\n",
    "    matching_rows = refer_df[refer_df['Subreddit'] == subreddit_city]\n",
    "    \n",
    "    #Skip if this city no found. \n",
    "    if matching_rows.empty:\n",
    "        print(f\"No matching subreddit found for {subreddit_city}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    #Skip for city that labeled null on refer list.\n",
    "    row = matching_rows.iloc[0]\n",
    "    if row.isin(['null']).any():\n",
    "        print(f\"Skipping {subreddit_city} due to null values.\")\n",
    "        continue\n",
    "        \n",
    "    df['n_posts'] = pd.to_numeric(df['n_posts'], errors='coerce')\n",
    "    \n",
    "    #Group by author to ensure all authors are unique\n",
    "    grouped_df = df.groupby('author')['n_posts'].sum().reset_index()\n",
    "    \n",
    "    #Add geographical info.\n",
    "    grouped_df['Place_Name'] = row['Place_Name']\n",
    "    grouped_df['State_Name'] = row['State_Name']\n",
    "    \n",
    "    #Append to df list for futher concate\n",
    "    df_list.append(grouped_df)\n",
    "\n",
    "#Concate all df with unique authors\n",
    "max_posts_df = pd.concat(df_list)\n",
    "max_posts_df['n_posts'] = max_posts_df['n_posts'].astype(int)\n",
    "\n",
    "#Group by author to sum their posts with unique geographical info\n",
    "max_posts_df = max_posts_df.groupby(['author',\n",
    "                                     'Place_Name',\n",
    "                                     'State_Name'])['n_posts'].sum().reset_index()\n",
    "\n",
    "#Select rows with max posts for author with unique geographical \n",
    "#Using .transform() to keep all equal max posts rows.\n",
    "max_posts_df = max_posts_df[max_posts_df.groupby('author')['n_posts']\n",
    "                            .transform(max) == max_posts_df['n_posts']]\n",
    "max_posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6114b7",
   "metadata": {},
   "source": [
    "## 3. Label for Duplicated Data and Separate fron Unique Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c966b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 unique authors after first process\n",
      "                author  Place_Name State_Name  n_posts  duplicated_author  \\\n",
      "0              IhateS2  test city1         S1       99               True   \n",
      "1              IhateS2  test city2         S2       99               True   \n",
      "2            IliveInS1  test city1         S1      100              False   \n",
      "4   IliveInS1&neverlft  test city0         S1       99               True   \n",
      "5   IliveInS1&neverlft  test city1         S1       99               True   \n",
      "6            IliveInS2  test city1         S1       99               True   \n",
      "7            IliveInS2  test city2         S2       99               True   \n",
      "8              random1  test city0         S1       99              False   \n",
      "9              random2  test city0         S1       99              False   \n",
      "10             random3  test city0         S1       99              False   \n",
      "\n",
      "           Moves  \n",
      "0   out-of-state  \n",
      "1   out-of-state  \n",
      "2           None  \n",
      "4    intra-state  \n",
      "5    intra-state  \n",
      "6   out-of-state  \n",
      "7   out-of-state  \n",
      "8           None  \n",
      "9           None  \n",
      "10          None  \n",
      "Duplicated Authors Dataframe:\n",
      "               author  Place_Name State_Name  n_posts         Moves\n",
      "0             IhateS2  test city1         S1       99  out-of-state\n",
      "1             IhateS2  test city2         S2       99  out-of-state\n",
      "4  IliveInS1&neverlft  test city0         S1       99   intra-state\n",
      "5  IliveInS1&neverlft  test city1         S1       99   intra-state\n",
      "6           IliveInS2  test city1         S1       99  out-of-state\n",
      "7           IliveInS2  test city2         S2       99  out-of-state\n",
      "Unique Authors Dataframe:\n",
      "       author  Place_Name State_Name  n_posts\n",
      "2   IliveInS1  test city1         S1      100\n",
      "8     random1  test city0         S1       99\n",
      "9     random2  test city0         S1       99\n",
      "10    random3  test city0         S1       99\n"
     ]
    }
   ],
   "source": [
    "def determine_equal_amount(row):\n",
    "    if row['duplicated_author']:\n",
    "        author_rows = max_posts_df[max_posts_df['author'] == row['author']]\n",
    "        if author_rows.duplicated('State_Name').any():\n",
    "            return 'intra-state'\n",
    "        else:\n",
    "            return 'out-of-state'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Extract unique rows after first process\n",
    "unique_authors_df = max_posts_df.drop_duplicates('author', keep=False).copy()\n",
    "print(f\"{len(unique_authors_df.author)} unique authors after first process\")\n",
    "\n",
    "#Label duplicated rows after first process in a temporary col\n",
    "max_posts_df['duplicated_author'] = max_posts_df.duplicated('author', keep=False)\n",
    "\n",
    "#Add Moves column to indicate their statue\n",
    "#out-of-state: posted in two or more state\n",
    "#intra-state: posted within one state only\n",
    "max_posts_df['Moves'] = max_posts_df.apply(determine_equal_amount, axis=1)\n",
    "print(max_posts_df)\n",
    "#Remove temporary row after process\n",
    "max_posts_df.drop(columns=['duplicated_author'], inplace=True)\n",
    "\n",
    "#Extract duplicated rows after labeling\n",
    "duplicated_authors_df = max_posts_df[max_posts_df.duplicated('author', keep=False)].copy()\n",
    "print(\"Duplicated Authors Dataframe:\")\n",
    "print(duplicated_authors_df)\n",
    "\n",
    "print(\"Unique Authors Dataframe:\")\n",
    "print(unique_authors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47362d7",
   "metadata": {},
   "source": [
    "## 4. Apply Majority Vote for\"out-of-State\" Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb089be",
   "metadata": {},
   "source": [
    "## rows_to_move = []\n",
    "\n",
    "#Select rows labeled 'out-of-state'\n",
    "out_state_rows = duplicated_authors_df[duplicated_authors_df['Moves'] == 'out-of-state']\n",
    "\n",
    "#Map the coresponding subreddit/file name to this list for futher uses.\n",
    "state_to_subreddit = state_refer_df.set_index('State_Name')['Subreddit'].to_dict()\n",
    "out_state_rows['Subreddit'] = out_state_rows['State_Name'].map(state_to_subreddit)\n",
    "\n",
    "#Proces each unique author\n",
    "for author in out_state_rows['author'].unique():\n",
    "    author_state_df = out_state_rows[out_state_rows['author'] == author]\n",
    "     \n",
    "    #Get their unique state lists\n",
    "    state_names = author_state_df['State_Name'].unique()\n",
    "    matching_rows = []\n",
    "    \n",
    "    #Proces each unique state within unique author to get all state level files. \n",
    "    for state_name in state_names:\n",
    "        subreddit = state_refer_df[state_refer_df['State_Name'] == state_name]['Subreddit'] \n",
    "\n",
    "        filename = f\"{subreddit.values[0]}_posts.csv\"\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        #Extract the author we need only from state files.\n",
    "        author_rows = df[df['author'] == author]\n",
    "        \n",
    "        #Added to a list\n",
    "        if not author_rows.empty:\n",
    "            matching_rows.append(author_rows)\n",
    "    \n",
    "    #if this list exist (not empty)\n",
    "    if matching_rows:\n",
    "        \n",
    "        #Concate all dfs with this author from state file\n",
    "        concatenated_df = pd.concat(matching_rows)\n",
    "        \n",
    "        #For only one state found\n",
    "        if concatenated_df.shape[0] == 1:\n",
    "            row = concatenated_df.iloc[0]\n",
    "            \n",
    "            #Match the row from state file with the coresponding row in duplicated df using unique author and subreddit info\n",
    "            row_to_keep = out_state_rows[(out_state_rows['Subreddit'] == row['subreddit'].lower()) & \n",
    "                                         (out_state_rows['author'] == row['author'])]\n",
    "            #Add to a list for futher concate\n",
    "            rows_to_move += [row_to_keep]\n",
    "            \n",
    "        #For more than one state found\n",
    "        else:\n",
    "            \n",
    "            #Find with state this author posted most\n",
    "            max_post_row = concatenated_df.loc[concatenated_df['n_posts'].idxmax()]\n",
    "            \n",
    "            #Match the row from state file with the coresponding row in duplicated df using unique author and subreddit info\n",
    "            max_row_to_keep = out_state_rows[(out_state_rows['Subreddit'] == max_post_row['subreddit'].lower()) & \n",
    "                                         (out_state_rows['author'] == max_post_row['author'])]\n",
    "            \n",
    "            #Add to a list for futher concate\n",
    "            rows_to_move += [max_row_to_keep]\n",
    "\n",
    "#Concate all rows we want to keep\n",
    "rows_to_move_df = pd.concat(rows_to_move)\n",
    "\n",
    "#Remove the temporary column\n",
    "rows_to_move_df.drop(['Subreddit'], axis=1, inplace=True)\n",
    "\n",
    "#With all rows we want to keep:\n",
    "for author in rows_to_move_df['author'].unique():\n",
    "    \n",
    "    #Remove them from the duplicated df since they are no longer duplicated\n",
    "    duplicated_authors_df = duplicated_authors_df.drop(duplicated_authors_df[\n",
    "        duplicated_authors_df['author'] == author].index)\n",
    "    \n",
    "#Drop the moves to fit unique df\n",
    "rows_to_move_df.drop(['Moves'], axis=1, inplace=True)\n",
    "unique_authors_df = pd.concat([unique_authors_df, rows_to_move_df])\n",
    "\n",
    "intra_state = len(duplicated_authors_df[duplicated_authors_df[\"Moves\"]==\"intra-state\"])\n",
    "out_state = len(duplicated_authors_df[duplicated_authors_df[\"Moves\"]==\"out-of-state\"])\n",
    "\n",
    "print(f\"{len(unique_authors_df.author)} unique authors after second process\")\n",
    "print(f\"{int(intra_state/2)} intra-state authors after second process\")\n",
    "print(f\"{int(out_state/2)} out-of-state authors after second process\")\n",
    "\n",
    "print(\"Unique Authors Dataframe:\")\n",
    "print(unique_authors_df)\n",
    "print(\"Duplicated Authors Dataframe:\")\n",
    "print(duplicated_authors_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ec85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors_df.to_csv('Uniq_MaxPosts.csv', index=False)\n",
    "duplicated_authors_df.to_csv('Duplic_MaxPosts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11758ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
